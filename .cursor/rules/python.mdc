---
description:
globs:
alwaysApply: true
---
# Python Software Engineering Principles

Use Python version define in [pyproject.toml](mdc:pyproject.toml).

## Principles

### Clean Architecture

Use the principle of clean architecture in your realisation.

### Separation of Concerns

Use the principle of separation of concerns in your realisation.

### Dependency Injection

Use the principle of dependency injection in your realisation.

### Inversion of Control

Use the principle of inversion of control in your realisation.

### Single Responsibility

Use the principle of single responsibility in your realisation.

### Least Privilege

Use the principle of least privilege in your realisation.

## Development Environment

### Virtual Environment

This project uses a virtual environment located in `.venv` directory. The virtual environment should be created and activated before working on the project.

#### Creating Virtual Environment

The virtual environment is typically created automatically by Poetry when installing dependencies. If you need to create it manually:

```bash
python3.12 -m venv .venv
```

#### Activating Virtual Environment

**On Linux/macOS:**
```bash
source .venv/bin/activate
```

**On Windows:**
```bash
.venv\Scripts\activate
```

#### Deactivating Virtual Environment

```bash
deactivate
```

### Poetry

This project uses [Poetry](https://python-poetry.org/) for dependency management and packaging. Poetry manages dependencies defined in `pyproject.toml`.

#### Installing Poetry

If Poetry is not installed, follow the [official installation guide](https://python-poetry.org/docs/#installation).

#### Installing Dependencies

Install all dependencies including development dependencies:

```bash
poetry install --with test
```

This will:
- Create a virtual environment if it doesn't exist (in `.venv` directory)
- Install all production dependencies
- Install all test dependencies (from `[tool.poetry.group.test]`)

#### Adding Dependencies

**Production dependency:**
```bash
poetry add package-name
```

**Development dependency:**
```bash
poetry add --group test package-name
```

#### Updating Dependencies

Update all dependencies to their latest compatible versions:

```bash
poetry update --with test
```

This will update `poetry.lock` file with the latest compatible versions.

#### Synchronizing Dependencies

Ensure the virtual environment matches the lock file exactly:

```bash
poetry install --with test --sync
```

#### Running Commands in Virtual Environment

Poetry can run commands in the virtual environment without activating it:

```bash
poetry run pytest
poetry run mypy
poetry run ruff check src tests
```

#### Virtual Environment Location

By default, Poetry creates virtual environments in a centralized location. This project uses a local `.venv` directory in the project root. To configure Poetry to use a local virtual environment:

```bash
poetry config virtualenvs.in-project true
```

## Code Style and Formatting

- Maximum line length: 120 characters (as configured in [pyproject.toml](mdc:pyproject.toml))
- Use Black for code formatting (configured in [pyproject.toml](mdc:pyproject.toml))
- Use Ruff for linting and additional formatting (configured in [pyproject.toml](mdc:pyproject.toml))
- Use 4 spaces for indentation (no tabs)
- Use double quotes for strings (as configured in Ruff)
- Follow PEP 8 style guide, except where overridden by project configuration

## Import Organization

Organize imports in the following order, with blank lines between groups:

1. Standard library imports
2. Third-party imports
3. Local application/library imports

Within each group, sort imports alphabetically. Use absolute imports from the package root.

Example:
```python
import json
from http import HTTPStatus
from typing import Any, Generic, TypeVar

import aiohttp
from fastapi import Depends
from pydantic import BaseModel

from fastapi_factory_utilities.core.app import DependencyConfig
from fastapi_factory_utilities.core.exceptions import FastAPIFactoryUtilitiesError
```

## Naming Conventions

- **Classes**: PascalCase (e.g., `AbstractRepository`, `HydraIntrospectService`)
- **Functions and methods**: snake_case (e.g., `get_config`, `on_startup`)
- **Constants**: UPPER_SNAKE_CASE (e.g., `DEFAULT_LOGGING_LEVEL`, `INTROSPECT_ENDPOINT`)
- **Private attributes**: Leading underscore (e.g., `_name`, `_queue`, `_consumer_tag`)
- **Type variables**: Descriptive names with Generic suffix (e.g., `DocumentGenericType`, `EntityGenericType`)
- **Modules**: snake_case (e.g., `exceptions.py`, `services.py`)

## Typing

Always type everything you can (variables, functions, classes, etc.).

### Type Annotations

- Annotate all function parameters and return types
- Annotate all class attributes
- Annotate all variables when the type is not obvious from context
- Avoid using `Any` unless absolutely necessary
- Use `Self` for return type annotations when returning the instance (Python 3.11+)

### Generic Types

Use `TypeVar` for generic types with descriptive names:

```python
from typing import Generic, TypeVar

DocumentGenericType = TypeVar("DocumentGenericType", bound=BaseDocument)
EntityGenericType = TypeVar("EntityGenericType", bound=BaseModel)

class AbstractRepository(ABC, Generic[DocumentGenericType, EntityGenericType]):
    ...
```

### TypedDict

Use `TypedDict` for structured dictionary data with type safety:

```python
from typing import NotRequired, TypedDict, Unpack

class ExceptionParameters(TypedDict):
    """Parameters for the exception."""
    message: NotRequired[str]
    level: NotRequired[int]

def __init__(self, *args: object, **kwargs: Unpack[ExceptionParameters]) -> None:
    ...
```

### Collections

Use `collections.abc` types for abstract base classes:

```python
from collections.abc import AsyncGenerator, Awaitable, Callable, Mapping
```

## Async/Await Patterns

### Async Functions

- Use `async def` for all I/O operations (database, HTTP, message queues, etc.)
- Use `async def` for functions that call other async functions
- Prefer async context managers (`async with`) for resource management

### Context Managers

Use `asynccontextmanager` for async context managers:

```python
from contextlib import asynccontextmanager
from collections.abc import AsyncGenerator

@asynccontextmanager
async def fastapi_lifespan(self, fastapi: FastAPI) -> AsyncGenerator[None, None]:
    await self.startup_plugins()
    await self.on_startup()
    try:
        yield
    finally:
        await self.on_shutdown()
        await self.shutdown_plugins()
```

### Parallel Operations

Use `asyncio.TaskGroup` (Python 3.11+) for parallel async operations:

```python
from asyncio import TaskGroup

async with TaskGroup() as tg:
    tg.create_task(self._verify(jwt_raw=jwt_raw), name="verify_jwt")
    task_decode: Task[Any] = tg.create_task(self._decode_jwt(jwt_raw=jwt_raw), name="decode_jwt")
```

### Async Generators

Use `AsyncGenerator` for async generator functions:

```python
from collections.abc import AsyncGenerator

async def fetch_items() -> AsyncGenerator[Item, None]:
    async for item in database.fetch():
        yield item
```

### Exception Handling

Handle exceptions in async code properly, ensuring resources are cleaned up:

```python
async def operation(self) -> None:
    try:
        await self._perform_operation()
    except SpecificError as e:
        await self._cleanup()
        raise
```

## Logging Patterns

### Structlog Usage

- Use `structlog` for all logging operations
- Get logger using `get_logger()` from `structlog.stdlib`
- Use `BoundLogger` type for logger variables

```python
from structlog.stdlib import BoundLogger, get_logger

_logger: BoundLogger = get_logger()
```

### Logging Levels

Use appropriate logging levels:
- `DEBUG`: Detailed information for diagnosing problems
- `INFO`: General informational messages
- `WARNING`: Warning messages for potentially harmful situations
- `ERROR`: Error messages for serious problems
- `CRITICAL`: Critical errors that may cause the application to stop

### Structured Logging

Use structured logging with context:

```python
_logger.log(level=logging.ERROR, event="Operation failed", operation="create_user", user_id=user_id)
```

### OpenTelemetry Integration

Integrate logging with OpenTelemetry spans when available:

```python
from opentelemetry.trace import Span, get_current_span

span: Span = get_current_span()
if span.is_recording():
    span.record_exception(exception)
    span.set_attribute("key", value)
```

## Error Handling

### Custom Exception Hierarchy

All custom exceptions should extend `FastAPIFactoryUtilitiesError`:

```python
from fastapi_factory_utilities.core.exceptions import FastAPIFactoryUtilitiesError

class CustomError(FastAPIFactoryUtilitiesError):
    """Description of the custom error."""
    DEFAULT_LOGGING_LEVEL: int = logging.ERROR
    DEFAULT_MESSAGE: str | None = None
```

### Exception Parameters

Use `TypedDict` with `Unpack` for exception parameters:

```python
from typing import NotRequired, TypedDict, Unpack

class ExceptionParameters(TypedDict):
    """Parameters for the exception."""
    message: NotRequired[str]
    level: NotRequired[int]

def __init__(self, *args: object, **kwargs: Unpack[ExceptionParameters]) -> None:
    ...
```

### Exception Logging and OpenTelemetry

Exceptions should:
- Log themselves using the configured logger
- Integrate with OpenTelemetry spans when available
- Include relevant context in span attributes

### Exception Chaining

Use proper exception chaining when re-raising exceptions:

```python
try:
    result = await operation()
except ValueError as e:
    raise CustomError("Operation failed") from e
```

## Pydantic Patterns

### BaseModel Usage

- Use Pydantic v2 `BaseModel` for all data models
- Use `model_validate` and `model_validate_json` for validation
- Use `model_dump` and `model_dump_json` for serialization

```python
from pydantic import BaseModel

class UserModel(BaseModel):
    """User model."""
    id: str
    name: str
    email: str

# Validation
user = UserModel.model_validate({"id": "1", "name": "John", "email": "john@example.com"})
user = UserModel.model_validate_json('{"id": "1", "name": "John", "email": "john@example.com"}')

# Serialization
data = user.model_dump()
json_data = user.model_dump_json()
```

### Field Configuration

Use Pydantic Field for advanced configuration:

```python
from pydantic import BaseModel, Field

class ConfigModel(BaseModel):
    """Configuration model."""
    timeout: float = Field(default=10.0, gt=0, description="Operation timeout in seconds")
    retries: int = Field(default=3, ge=0, le=10, description="Number of retry attempts")
```

### Validation Patterns

Implement custom validators when needed:

```python
from pydantic import BaseModel, BeforeValidator
from typing import Annotated

def ensure_logging_level(level: Any) -> int:
    """Ensure the logging level."""
    if isinstance(level, int):
        return level
    if isinstance(level, str):
        return getattr(logging, str(level).upper())
    raise ValueError(f"Invalid logging level: {level}")

class LoggingConfig(BaseModel):
    """Logging configuration."""
    level: Annotated[int, BeforeValidator(ensure_logging_level)]
```

## Testing

### Test Organization

- Use pytest for all tests
- Organize tests by functional scope in classes
- Prioritize unit tests over integration tests
- Place tests in `tests/units/` for unit tests
- Use descriptive test class names (e.g., `TestSetupLog`, `TestExceptions`)

### Test Structure

```python
class TestFeatureName:
    """Various tests for the feature_name function."""

    def test_specific_scenario(self) -> None:
        """Test that specific scenario works correctly."""
        # Arrange
        # Act
        # Assert
```

### Async Tests

Use `pytest-asyncio` for async tests. The project is configured with `asyncio_mode = "auto"`:

```python
import pytest

class TestAsyncFeature:
    """Tests for async features."""

    async def test_async_operation(self) -> None:
        """Test async operation."""
        result = await async_function()
        assert result is not None
```

### Parametrized Tests

Use `pytest.mark.parametrize` for testing multiple scenarios:

```python
import pytest

class TestValidation:
    """Tests for validation logic."""

    @pytest.mark.parametrize(
        "input_value,expected",
        [
            ("valid", True),
            ("invalid", False),
            ("", False),
        ],
    )
    def test_validation(self, input_value: str, expected: bool) -> None:
        """Test validation with different inputs."""
        result = validate(input_value)
        assert result == expected
```

### Mocking

Use `unittest.mock` for mocking dependencies:

```python
from unittest.mock import MagicMock, patch

class TestService:
    """Tests for service."""

    @patch("module.external_dependency")
    def test_with_mock(self, mock_dependency: MagicMock) -> None:
        """Test with mocked dependency."""
        mock_dependency.return_value = "expected"
        result = function_under_test()
        assert result == "expected"
        mock_dependency.assert_called_once()
```

### Test Naming

- Test class names: `TestFeatureName` or `TestClassName`
- Test method names: `test_specific_scenario_description`
- Use descriptive names that explain what is being tested

## Documentation

### Docstring Format

Always add docstrings to methods, functions, and classes using Google Python docstring format.

### Module-Level Docstrings

Every module should start with a module-level docstring:

```python
"""Provides functionality for X."""
```

### Class Docstrings

```python
class MyClass:
    """Brief description of the class.

    Longer description if needed, explaining the purpose and usage
    of the class.
    """
```

### Function/Method Docstrings

```python
def my_function(param1: str, param2: int) -> bool:
    """Brief description of the function.

    Longer description if needed, explaining what the function does
    and any important details.

    Args:
        param1: Description of param1.
        param2: Description of param2.

    Returns:
        Description of the return value.

    Raises:
        ValueError: Description of when this exception is raised.
        CustomError: Description of when this exception is raised.
    """
```

### Complex Functions

For complex functions, include examples in the docstring:

```python
def complex_function(data: dict[str, Any]) -> ProcessedData:
    """Process complex data structure.

    This function performs multiple transformations on the input data
    and returns a processed result.

    Args:
        data: Dictionary containing raw data to process.

    Returns:
        ProcessedData object with transformed data.

    Example:
        >>> data = {"key": "value"}
        >>> result = complex_function(data)
        >>> print(result.processed_key)
        'processed_value'

    Raises:
        ValidationError: If the input data is invalid.
    """
```

## Architecture Patterns

### Abstract Base Classes

Use `ABC` and `abstractmethod` for interfaces:

```python
from abc import ABC, abstractmethod

class AbstractRepository(ABC, Generic[DocumentGenericType, EntityGenericType]):
    """Abstract repository interface."""

    @abstractmethod
    async def create(self, entity: EntityGenericType) -> DocumentGenericType:
        """Create a new entity."""
        raise NotImplementedError
```

### Plugin Pattern

Implement plugins by extending `PluginAbstract`:

```python
from fastapi_factory_utilities.core.plugins.abstracts import PluginAbstract

class MyPlugin(PluginAbstract):
    """Plugin implementation."""

    async def setup(self) -> Self:
        """Setup the plugin."""
        # Initialization logic
        return self

    async def shutdown(self) -> None:
        """Shutdown the plugin."""
        # Cleanup logic
```

### Dependency Injection

Use FastAPI's dependency injection system:

```python
from fastapi import Depends
from fastapi_factory_utilities.core.app import depends_dependency_config

async def my_endpoint(
    config: DependencyConfig = Depends(depends_dependency_config),
) -> dict[str, Any]:
    """Endpoint using dependency injection."""
    return {"config": config}
```

### Repository Pattern

Implement repositories for data access:

```python
class UserRepository(AbstractRepository[UserDocument, UserEntity]):
    """Repository for user entities."""

    async def find_by_email(self, email: str) -> UserDocument | None:
        """Find user by email."""
        return await self.find_one(UserDocument.email == email)
```

### Service Layer

Implement service classes for business logic:

```python
class UserService:
    """Service for user operations."""

    def __init__(self, repository: UserRepository) -> None:
        """Initialize the service."""
        self._repository: UserRepository = repository

    async def create_user(self, user_data: UserEntity) -> UserDocument:
        """Create a new user."""
        # Business logic here
        return await self._repository.create(user_data)
```

## File Structure

### Module Organization

- Organize code by feature/domain, not by technical layer
- Keep related functionality together
- Use clear, descriptive module names

### Package Structure

```
src/
  fastapi_factory_utilities/
    core/
      app/
        __init__.py
        application.py
      exceptions.py
      services/
        hydra/
          __init__.py
          services.py
          exceptions.py
```

### `__init__.py` Patterns

Use `__init__.py` to expose public API:

```python
"""Package description."""

from .application import ApplicationAbstract
from .config import RootConfig

__all__ = ["ApplicationAbstract", "RootConfig"]
```

### Separation of Concerns

- Keep business logic separate from infrastructure code
- Separate interfaces (abstract classes) from implementations
- Keep configuration separate from application code
- Separate exceptions into their own modules when they grow large

## How to Test

### Running Tests

Run all tests using pytest:

```bash
pytest
```

Or using Poetry:

```bash
poetry run pytest
```

### Running Specific Tests

**Run tests in a specific directory:**
```bash
pytest tests/units
```

**Run tests in a specific file:**
```bash
pytest tests/units/test_exceptions.py
```

**Run a specific test class:**
```bash
pytest tests/units/test_exceptions.py::TestExceptions
```

**Run a specific test method:**
```bash
pytest tests/units/test_exceptions.py::TestExceptions::test_specific_scenario
```

### Running Tests with Coverage

Generate coverage reports:

```bash
pytest --cov=src --cov-report=html --cov-report=term
```

This will:
- Generate coverage data for the `src` directory
- Create an HTML report in `htmlcov/` directory
- Display coverage summary in the terminal

**Generate LCOV format (for CI/CD):**
```bash
pytest --cov=src --cov-report=lcov:build/coverage.lcov
```

**Generate JUnit XML (for CI/CD):**
```bash
pytest --junitxml=build/junit.xml
```

### Running Tests in Parallel

The project is configured to run tests in parallel automatically using `pytest-xdist`:

```bash
pytest -n auto
```

The `-n auto` flag automatically detects the number of CPU cores and runs tests in parallel.

### Running Tests with Verbose Output

Get more detailed output:

```bash
pytest -v
```

Or even more verbose:

```bash
pytest -vv
```

### Running Tests with Output Capture Disabled

See print statements and logs during test execution:

```bash
pytest -s
```

### Running Tests Matching a Pattern

Run tests matching a specific pattern:

```bash
pytest -k "test_exception"
```

This will run all tests with "test_exception" in their name.

### Running Tests and Stopping on First Failure

Stop test execution on the first failure:

```bash
pytest -x
```

Or stop after N failures:

```bash
pytest --maxfail=3
```

### Running Tests with Markers

Run tests with specific markers:

```bash
pytest -m "not slow"
```

## How to Use Pre-commit

### Installing Pre-commit Hooks

Install pre-commit hooks to run automatically on git commits:

```bash
pre-commit install
```

This will install hooks for the `pre-commit` stage. To also install hooks for the `pre-push` stage:

```bash
pre-commit install --hook-type pre-push
```

### Running Pre-commit Hooks Manually

Run all pre-commit hooks on all files:

```bash
pre-commit run --all-files
```

Run a specific hook:

```bash
pre-commit run ruff-check --all-files
pre-commit run pytest --all-files
pre-commit run mypy --all-files
```

### Running Pre-commit Hooks on Staged Files

Run hooks only on files that are staged for commit:

```bash
pre-commit run
```

### Pre-commit Hooks Configuration

The project uses the following pre-commit hooks (configured in `.pre-commit-config.yaml`):

#### Code Formatting and Linting

- **ruff-format**: Formats code using Ruff
- **ruff-check**: Checks code style and fixes issues automatically
- **trailing-whitespace**: Removes trailing whitespace
- **end-of-file-fixer**: Ensures files end with a newline

#### Type Checking

- **mypy**: Performs static type checking

#### Code Quality

- **pylint**: Performs additional code quality checks

#### Testing

- **pytest**: Runs unit tests with coverage

#### Dependency Management

- **poetry lock and update**: Updates and syncs Poetry lock file (runs on pre-push only)

### Pre-commit Hook Stages

Hooks run at different stages:

- **pre-commit**: Runs when you execute `git commit`
- **pre-push**: Runs when you execute `git push`
- **manual**: Can be run manually using `pre-commit run`

### Skipping Pre-commit Hooks

To skip pre-commit hooks for a specific commit (not recommended):

```bash
git commit --no-verify
```

### Updating Pre-commit Hooks

Update pre-commit hooks to their latest versions:

```bash
pre-commit autoupdate
```

This updates the hook versions in `.pre-commit-config.yaml` to the latest available versions.

### Pre-commit Cache

Pre-commit caches hook environments. To clear the cache:

```bash
pre-commit clean
```
